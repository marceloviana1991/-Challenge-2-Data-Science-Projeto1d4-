!pip install pyspark==3.3.1

from pyspark.sql import SparkSession
from pyspark.sql import functions as f

spark = SparkSession.builder.master('local[*]').appName("Transformacao com Spark").getOrCreate()

!wget "https://caelum-online-public.s3.amazonaws.com/challenge-spark/semana-1.zip" && unzip semana-1.zip -d dados/

dados = spark.read.json('/content/dados/dataset_bruto.json')

#Verificando a configuração do data frame
dados.show(5, truncate=False)
dados.printSchema()

#Verificando a quantidade de registros
dados.count()

#Filtrando o Data Frame para eliminar as colunas 'imagens' e 'usuario'
anuncio = dados.select('anuncio.*')

#Analisando os dados {'tipo_uso':['Comercial', 'Residencial'], 'tipo_anuncio':['Usado', 'Lançamento'], 'tipo_unidade':['Outros', 'Apartamento', 'Casa']}
anuncio.groupBy('tipo_uso').count().show()
anuncio.groupBy('tipo_anuncio').count().show()
anuncio.groupBy('tipo_unidade').count().show()

#Filtrando o Data Frame para elimenar os dados {'tipo_uso':['Residencial'], 'tipo_anuncio':['Lançamento'], 'tipo_unidade':['Outros', 'Casa']}
anuncio = anuncio.filter("tipo_uso == 'Residencial'")
anuncio = anuncio.filter("tipo_unidade == 'Apartamento'")
anuncio = anuncio.filter("tipo_anuncio == 'Usado'")

#Analisando os a quantidade de elementos das listas registradas nas colunas 'quartos', 'suites', 'banheiros', 'vaga', 'area_total', 'area_util'
caracteristicas = ['quartos', 'suites', 'banheiros', 'vaga', 'area_total', 'area_util']
for i in caracteristicas:
  anuncio.select(f.size(f.col(i)).alias(i)).groupBy(i).count().show()
  
#Convertendo os registros das colunas 'quartos', 'suites', 'banheiros', 'vaga', 'area_total', 'area_util' de lista para elemento
#OBS: No caso de listas com 1 elemento registra-se o elemento; No caso de listas vazias registra-se null
anuncio = anuncio.select([f.col(c)[0].alias(c) if c in caracteristicas else c for c in anuncio.columns])
anuncio.show()
